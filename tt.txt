# AI Agents Framework for Literature Data Extraction

"""
This module provides a skeleton for a full working framework that:
1. Crawls scientific literature sources (e.g., arXiv, PubMed) using Scrapy.
2. Parses and extracts PDF text with PDFMiner and BeautifulSoup.
3. Uses a LangChain-based multi-agent pipeline for instruction-driven extraction.
4. Outputs structured data and research ideation.
5. Automates CI/CD deployment with Docker and GitHub Actions.

Project structure:

literature_agent/
├── crawler/
│   ├── __init__.py
│   ├── spiders/
│   │   └── arxiv_spider.py
│   └── pipeline.py
├── extractor/
│   ├── __init__.py
│   ├── pdf_parser.py
│   └── web_parser.py
├── agent/
│   ├── __init__.py
│   ├── llm_agent.py
│   └── planner.py
├── config.yaml
├── main.py
└── requirements.txt
"""

# config.yaml (YAML configuration for sources, schema, and LLM settings)
# ---------------------------------------------------------------
sources:
  - name: arxiv
    start_urls:
      - https://arxiv.org/list/physics/new
  - name: pubmed
    start_urls:
      - https://pubmed.ncbi.nlm.nih.gov/?term=machine+learning

extraction:
  schema:
    - title
    - authors
    - abstract
    - key_results
    - methods

llm:
  model: "openai/gpt-4"
  temperature: 0.2
  chunk_size: 1000

output:
  format: json
  path: data/output.json

# requirements.txt
# ---------------------------------------------------------------
scrapy
pdfminer.six
beautifulsoup4
langchain
openai
PyYAML

# crawler/spiders/arxiv_spider.py
# ---------------------------------------------------------------
import scrapy

class ArxivSpider(scrapy.Spider):
    name = "arxiv"
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'CONCURRENT_REQUESTS': 4,
    }
    def start_requests(self):
        urls = self.settings.get('sources')[0]['start_urls']
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for entry in response.css('div.listing .list-title'):
            pdf_url = entry.xpath('../a[@title="Download PDF"]/attribute::href').get()
            yield response.follow(pdf_url, self.parse_pdf)

    def parse_pdf(self, response):
        path = self.settings.get('output')['path'].replace('.json', '') + '_tmp.pdf'
        with open(path, 'wb') as f:
            f.write(response.body)
        yield {'file_path': path}

# crawler/pipeline.py
# ---------------------------------------------------------------
from scrapy.crawler import CrawlerProcess
import yaml

def run_crawler():
    with open('config.yaml') as f:
        settings = yaml.safe_load(f)
    process = CrawlerProcess(settings={**settings, 'FEED_FORMAT': 'json', 'FEED_URI': 'crawled.json'})
    process.crawl('arxiv')
    process.start()

# extractor/pdf_parser.py
# ---------------------------------------------------------------
from pdfminer.high_level import extract_text

def parse_pdf(file_path):
    text = extract_text(file_path)
    # simple split into chunks
    return text

# agent/llm_agent.py
# ---------------------------------------------------------------
from langchain import OpenAI, LLMChain, PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

class ExtractionAgent:
    def __init__(self, config):
        self.llm = OpenAI(model_name=config['llm']['model'], temperature=config['llm']['temperature'])
        self.splitter = RecursiveCharacterTextSplitter(chunk_size=config['llm']['chunk_size'])
        self.prompt = PromptTemplate(
            input_variables=['chunk','schema'],
            template="""
Extract the following schema fields from the text chunk:
{schema}

Text:
{chunk}

Return JSON only.
"""
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def extract(self, text, schema):
        chunks = self.splitter.split_text(text)
        results = []
        for chunk in chunks:
            res = self.chain.run(chunk=chunk, schema=schema)
            results.append(res)
        # merge partial JSONs
        return results

# agent/planner.py
# ---------------------------------------------------------------
from .llm_agent import ExtractionAgent
import yaml
import json

class AgentPipeline:
    def __init__(self, config_path='config.yaml'):
        with open(config_path) as f:
            self.config = yaml.safe_load(f)
        self.agent = ExtractionAgent(self.config)

    def run(self, crawled_items_path='crawled.json', output_path=None):
        items = json.load(open(crawled_items_path))
        all_results = []
        for item in items:
            text = parse_pdf(item['file_path'])
            res = self.agent.extract(text, self.config['extraction']['schema'])
            all_results.extend(res)
        out = output_path or self.config['output']['path']
        json.dump(all_results, open(out, 'w'), indent=2)

# main.py
# ---------------------------------------------------------------
from crawler.pipeline import run_crawler
from agent.planner import AgentPipeline

if __name__ == '__main__':
    # Step 1: Crawl
    run_crawler()
    # Step 2: AGent pipeline
    pipeline = AgentPipeline()
    pipeline.run()
    print("Extraction complete. Results in data/output.json")
